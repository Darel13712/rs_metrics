{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to rs_metrics This package implements metrics common in recommendation systems. All metrics are paralleled by users. Installation pip install rs_metrics Data Format Metrics expect to get a dict mapping user_id with list of item_id . Example pred = { 1 : [ 1 , 2 , 3 ], 2 : [ 3 , 2 , 4 ]} You can also pass pandas.DataFrame , which will be converted dict to automatically with convert_to_pandas function. Default columns are user_col='user_id', item_col='item_id' .","title":"Welcome to rs_metrics"},{"location":"#welcome-to-rs_metrics","text":"This package implements metrics common in recommendation systems. All metrics are paralleled by users.","title":"Welcome to rs_metrics"},{"location":"#installation","text":"pip install rs_metrics","title":"Installation"},{"location":"#data-format","text":"Metrics expect to get a dict mapping user_id with list of item_id . Example pred = { 1 : [ 1 , 2 , 3 ], 2 : [ 3 , 2 , 4 ]} You can also pass pandas.DataFrame , which will be converted dict to automatically with convert_to_pandas function. Default columns are user_col='user_id', item_col='item_id' .","title":"Data Format"},{"location":"metrics/","text":"Metrics Relevance HitRate def hitrate ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): Shows what percentage of users has at least one relevant recommendation in their list. HitRate@k = \\frac {\\sum \\limits_{u \\in U} \\max \\limits_{i \\in L(u)}(rel(i, u))} {|U|} HitRate@k = \\frac {\\sum \\limits_{u \\in U} \\max \\limits_{i \\in L(u)}(rel(i, u))} {|U|} rel(i, u) rel(i, u) equals 1 if item i i is relevant to user u u . L(u) L(u) is a recommendation list of length k k for user u u . Precision def precision ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): Shows what percentage of items in recommendations are relevant, on average. Precision@k = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|rel_u \\cap rec_u|}{|rec_u|} Precision@k = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|rel_u \\cap rec_u|}{|rec_u|} rel_u rel_u \u2013 items relevant for user u u rec_u rec_u \u2013 top-k items recommended to user u u Mean Average Precision def mapr ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): AP@k(u) = \\frac{1}{k} \\sum_{i \\in rec_u} rel(i, u)Precision@pos_{i, u}(u) AP@k(u) = \\frac{1}{k} \\sum_{i \\in rec_u} rel(i, u)Precision@pos_{i, u}(u) MAP@k = \\frac{1}{|U|} \\sum_{u \\in U} AP@k(u) MAP@k = \\frac{1}{|U|} \\sum_{u \\in U} AP@k(u) rel(i, u) rel(i, u) \u2013 equals 1 if item i i is relevant to user u u rec_u rec_u \u2013 top-k recommendations for user u u pos_{i, u} pos_{i, u} \u2013 position of item i i in recommendation list rec_u rec_u Recall def recall ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): Shows what percentage of relevant items appeared in recommendations, on average. Recall@k = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|rel_u \\cap rec_u|}{|rel_u|} Recall@k = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|rel_u \\cap rec_u|}{|rel_u|} rel_u rel_u \u2013 items relevant for user u u rec_u rec_u \u2013 top-k items recommended to user u u Mean Average Recall def mar ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): AR@k(u) = \\frac{1}{k} \\sum_{i \\in rec_u} rel(i, u)Recall@pos_{i, u}(u) AR@k(u) = \\frac{1}{k} \\sum_{i \\in rec_u} rel(i, u)Recall@pos_{i, u}(u) MAR@k = \\frac{1}{|U|} \\sum_{u \\in U} AR@k(u) MAR@k = \\frac{1}{|U|} \\sum_{u \\in U} AR@k(u) rel(i, u) rel(i, u) \u2013 equals 1 if item i i is relevant to user u u rec_u rec_u \u2013 top-k recommendations for user u u pos_{i, u} pos_{i, u} \u2013 position of item i i in recommendation list rec_u rec_u Ranking NDCG def ndcg ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): Normalized discounted cumulative gain is a ranking quality metric. It takes position of relevant items into account, promoting them to the beginning of the list. Although relevance can be any function (star rating), simple binary variant is usually used: DCG@k = \\sum_{i=1}^k \\frac{2^{rel(i)}-1} {log_2(i+1)} DCG@k = \\sum_{i=1}^k \\frac{2^{rel(i)}-1} {log_2(i+1)} NDCG@k = \\frac{DCG@k} {IDCG@k} NDCG@k = \\frac{DCG@k} {IDCG@k} rel(i) rel(i) equals 1 if item at position i i is relevant, i.e pred[u][i] \\in \\in true [ u ] . IDCG@k = \\max(DCG@k) IDCG@k = \\max(DCG@k) This value is averaged across all users. MRR def mrr ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): Mean Reciprocal Rank shows inverted position of the first relevant item, on average. Diversity \u03b1-NDCG def a_ndcg ( true , pred , aspects , k = 10 , alpha = 0.5 , user_col = 'user_id' , item_col = 'item_id' ): aspects : dictionary which maps users to aspect list containing items for each aspect. Example: { 1 : [{ 1 , 2 , 3 }, { 3 , 4 }], 2 : [{ 1 }]} alpha \\in [0,1] \\in [0,1] : controls redundancy penalty. The bigger the number the more metric penalizes items from the same aspect. \u03b1\\text{-}NDCG \u03b1\\text{-}NDCG is based on NDCG NDCG but it is aspect and redundancy-aware, which makes it a measure of diversity: \\alpha \\text{-}DCG@k=\\sum_{i=1}^k \\frac{1}{log_2(i + 1)} \\sum_{a \\in \\mathcal{A}} rel(i|a) \\prod_{j < i}(1-\\alpha rel(j|a)) \\alpha \\text{-}DCG@k=\\sum_{i=1}^k \\frac{1}{log_2(i + 1)} \\sum_{a \\in \\mathcal{A}} rel(i|a) \\prod_{j < i}(1-\\alpha rel(j|a)) \\alpha \\text{-}NDCG@k = \\frac{\\alpha \\text{-}DCG@k} {\\alpha \\text{-}IDCG@k} \\alpha \\text{-}NDCG@k = \\frac{\\alpha \\text{-}DCG@k} {\\alpha \\text{-}IDCG@k} a \\in A a \\in A are aspects of items i.e. features or subprofiles rel(i|a) rel(i|a) equals 1 if item at position i i is relevant and has aspect a a and 0 otherwise \\alpha \\text{-}IDCG@k = \\max(\\alpha \\text{-}DCG@k) \\alpha \\text{-}IDCG@k = \\max(\\alpha \\text{-}DCG@k) Other Coverage Shows what percentage of items from log appears in recommendations. def coverage ( items , recs , k = None , user_col = 'user_id' , item_col = 'item_id' ): items : list of unique item ids. If recommendations contain new items, not from items , metric won't be correct. recs : standard user-items dictionary or DataFrame k : pass specific k to limit the amount of visible recommendations for each user. Popularity Shows mean popularity of recommendations. Scores for items are averaged per recommendation list and globally. def popularity ( log , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): log : pandas DataFrame with interactions pred : dict of recommendations or DataFrame k : top k items to use from recs user_col : column name for user ids item_col : column name for item ids Surprisal Measures unexpectedness of recommendations. Let p_k p_k be popularity of item k, p_k \\in [0, 1] k, p_k \\in [0, 1] . Then we can introduce self-information for item k k as I_k = -log_2(p_k) I_k = -log_2(p_k) We calculate mean self-information for items in recommendation for each user and average it for all users. In a way it is opposite to the popularity metric. def surprisal ( log , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): log : pandas DataFrame with interactions pred : dict of recommendations or DataFrame k : top k items to use from recs user_col : column name for user ids item_col : column name for item ids","title":"Metrics"},{"location":"metrics/#metrics","text":"","title":"Metrics"},{"location":"metrics/#relevance","text":"","title":"Relevance"},{"location":"metrics/#hitrate","text":"def hitrate ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): Shows what percentage of users has at least one relevant recommendation in their list. HitRate@k = \\frac {\\sum \\limits_{u \\in U} \\max \\limits_{i \\in L(u)}(rel(i, u))} {|U|} HitRate@k = \\frac {\\sum \\limits_{u \\in U} \\max \\limits_{i \\in L(u)}(rel(i, u))} {|U|} rel(i, u) rel(i, u) equals 1 if item i i is relevant to user u u . L(u) L(u) is a recommendation list of length k k for user u u .","title":"HitRate"},{"location":"metrics/#precision","text":"def precision ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): Shows what percentage of items in recommendations are relevant, on average. Precision@k = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|rel_u \\cap rec_u|}{|rec_u|} Precision@k = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|rel_u \\cap rec_u|}{|rec_u|} rel_u rel_u \u2013 items relevant for user u u rec_u rec_u \u2013 top-k items recommended to user u u","title":"Precision"},{"location":"metrics/#mean-average-precision","text":"def mapr ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): AP@k(u) = \\frac{1}{k} \\sum_{i \\in rec_u} rel(i, u)Precision@pos_{i, u}(u) AP@k(u) = \\frac{1}{k} \\sum_{i \\in rec_u} rel(i, u)Precision@pos_{i, u}(u) MAP@k = \\frac{1}{|U|} \\sum_{u \\in U} AP@k(u) MAP@k = \\frac{1}{|U|} \\sum_{u \\in U} AP@k(u) rel(i, u) rel(i, u) \u2013 equals 1 if item i i is relevant to user u u rec_u rec_u \u2013 top-k recommendations for user u u pos_{i, u} pos_{i, u} \u2013 position of item i i in recommendation list rec_u rec_u","title":"Mean Average Precision"},{"location":"metrics/#recall","text":"def recall ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): Shows what percentage of relevant items appeared in recommendations, on average. Recall@k = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|rel_u \\cap rec_u|}{|rel_u|} Recall@k = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|rel_u \\cap rec_u|}{|rel_u|} rel_u rel_u \u2013 items relevant for user u u rec_u rec_u \u2013 top-k items recommended to user u u","title":"Recall"},{"location":"metrics/#mean-average-recall","text":"def mar ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): AR@k(u) = \\frac{1}{k} \\sum_{i \\in rec_u} rel(i, u)Recall@pos_{i, u}(u) AR@k(u) = \\frac{1}{k} \\sum_{i \\in rec_u} rel(i, u)Recall@pos_{i, u}(u) MAR@k = \\frac{1}{|U|} \\sum_{u \\in U} AR@k(u) MAR@k = \\frac{1}{|U|} \\sum_{u \\in U} AR@k(u) rel(i, u) rel(i, u) \u2013 equals 1 if item i i is relevant to user u u rec_u rec_u \u2013 top-k recommendations for user u u pos_{i, u} pos_{i, u} \u2013 position of item i i in recommendation list rec_u rec_u","title":"Mean Average Recall"},{"location":"metrics/#ranking","text":"","title":"Ranking"},{"location":"metrics/#ndcg","text":"def ndcg ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): Normalized discounted cumulative gain is a ranking quality metric. It takes position of relevant items into account, promoting them to the beginning of the list. Although relevance can be any function (star rating), simple binary variant is usually used: DCG@k = \\sum_{i=1}^k \\frac{2^{rel(i)}-1} {log_2(i+1)} DCG@k = \\sum_{i=1}^k \\frac{2^{rel(i)}-1} {log_2(i+1)} NDCG@k = \\frac{DCG@k} {IDCG@k} NDCG@k = \\frac{DCG@k} {IDCG@k} rel(i) rel(i) equals 1 if item at position i i is relevant, i.e pred[u][i] \\in \\in true [ u ] . IDCG@k = \\max(DCG@k) IDCG@k = \\max(DCG@k) This value is averaged across all users.","title":"NDCG"},{"location":"metrics/#mrr","text":"def mrr ( true , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): Mean Reciprocal Rank shows inverted position of the first relevant item, on average.","title":"MRR"},{"location":"metrics/#diversity","text":"","title":"Diversity"},{"location":"metrics/#-ndcg","text":"def a_ndcg ( true , pred , aspects , k = 10 , alpha = 0.5 , user_col = 'user_id' , item_col = 'item_id' ): aspects : dictionary which maps users to aspect list containing items for each aspect. Example: { 1 : [{ 1 , 2 , 3 }, { 3 , 4 }], 2 : [{ 1 }]} alpha \\in [0,1] \\in [0,1] : controls redundancy penalty. The bigger the number the more metric penalizes items from the same aspect. \u03b1\\text{-}NDCG \u03b1\\text{-}NDCG is based on NDCG NDCG but it is aspect and redundancy-aware, which makes it a measure of diversity: \\alpha \\text{-}DCG@k=\\sum_{i=1}^k \\frac{1}{log_2(i + 1)} \\sum_{a \\in \\mathcal{A}} rel(i|a) \\prod_{j < i}(1-\\alpha rel(j|a)) \\alpha \\text{-}DCG@k=\\sum_{i=1}^k \\frac{1}{log_2(i + 1)} \\sum_{a \\in \\mathcal{A}} rel(i|a) \\prod_{j < i}(1-\\alpha rel(j|a)) \\alpha \\text{-}NDCG@k = \\frac{\\alpha \\text{-}DCG@k} {\\alpha \\text{-}IDCG@k} \\alpha \\text{-}NDCG@k = \\frac{\\alpha \\text{-}DCG@k} {\\alpha \\text{-}IDCG@k} a \\in A a \\in A are aspects of items i.e. features or subprofiles rel(i|a) rel(i|a) equals 1 if item at position i i is relevant and has aspect a a and 0 otherwise \\alpha \\text{-}IDCG@k = \\max(\\alpha \\text{-}DCG@k) \\alpha \\text{-}IDCG@k = \\max(\\alpha \\text{-}DCG@k)","title":"\u03b1-NDCG"},{"location":"metrics/#other","text":"","title":"Other"},{"location":"metrics/#coverage","text":"Shows what percentage of items from log appears in recommendations. def coverage ( items , recs , k = None , user_col = 'user_id' , item_col = 'item_id' ): items : list of unique item ids. If recommendations contain new items, not from items , metric won't be correct. recs : standard user-items dictionary or DataFrame k : pass specific k to limit the amount of visible recommendations for each user.","title":"Coverage"},{"location":"metrics/#popularity","text":"Shows mean popularity of recommendations. Scores for items are averaged per recommendation list and globally. def popularity ( log , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): log : pandas DataFrame with interactions pred : dict of recommendations or DataFrame k : top k items to use from recs user_col : column name for user ids item_col : column name for item ids","title":"Popularity"},{"location":"metrics/#surprisal","text":"Measures unexpectedness of recommendations. Let p_k p_k be popularity of item k, p_k \\in [0, 1] k, p_k \\in [0, 1] . Then we can introduce self-information for item k k as I_k = -log_2(p_k) I_k = -log_2(p_k) We calculate mean self-information for items in recommendation for each user and average it for all users. In a way it is opposite to the popularity metric. def surprisal ( log , pred , k = 10 , user_col = 'user_id' , item_col = 'item_id' ): log : pandas DataFrame with interactions pred : dict of recommendations or DataFrame k : top k items to use from recs user_col : column name for user ids item_col : column name for item ids","title":"Surprisal"}]}